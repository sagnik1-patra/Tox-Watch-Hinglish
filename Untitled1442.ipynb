{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5d3bd5-6072-4656-b567-9a190d01e115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToxWatch Hinglish Artifacts Written ===\n",
      "H5:    C:\\Users\\sagni\\Downloads\\Tox Watch Hinglish\\toxwatch_hinglish.h5\n",
      "PKL:   C:\\Users\\sagni\\Downloads\\Tox Watch Hinglish\\toxwatch_hinglish.pkl\n",
      "YAML:  C:\\Users\\sagni\\Downloads\\Tox Watch Hinglish\\toxwatch_config.yaml\n",
      "JSONL: C:\\Users\\sagni\\Downloads\\Tox Watch Hinglish\\toxwatch_hinglish.jsonl\n",
      "SUM:   C:\\Users\\sagni\\Downloads\\Tox Watch Hinglish\\toxwatch_summary.json\n",
      "\n",
      "Sizes: {'train': 5728, 'valid': 811, 'test': 1653}\n",
      "Class balance: {'train': {'14': 3050, '8': 1009, '12': 478, '15': 405, '0': 305, '13': 163, '7': 81, '5': 74, '1': 34, '6': 28, '11': 28, '9': 27, '4': 24, '2': 9, '3': 9, '10': 4}, 'valid': {'14': 435, '8': 144, '12': 68, '15': 57, '0': 43, '13': 23, '7': 11, '5': 10, '11': 4, '6': 4, '1': 4, '4': 3, '9': 3, '2': 1, '3': 1}, 'test': {'14': 873, '8': 289, '12': 138, '15': 117, '0': 89, '13': 47, '7': 24, '5': 22, '1': 11, '6': 9, '9': 9, '4': 8, '11': 8, '2': 3, '10': 3, '3': 3}}\n",
      "Label2ID: {'defamation': 0, 'defamation,fake': 1, 'defamation,fake,hate': 2, 'defamation,fake,hate,offensive': 3, 'defamation,fake,offensive': 4, 'defamation,hate': 5, 'defamation,hate,offensive': 6, 'defamation,offensive': 7, 'fake': 8, 'fake,hate': 9, 'fake,hate,offensive': 10, 'fake,offensive': 11, 'hate': 12, 'hate,offensive': 13, 'non-hostile': 14, 'offensive': 15}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "    HAVE_YAML = True\n",
    "except Exception:\n",
    "    HAVE_YAML = False\n",
    "\n",
    "# --------- USER PATHS ----------\n",
    "TRAIN_PATH = r\"C:\\Users\\sagni\\Downloads\\Tox Watch Hinglish\\archive\\Hatespeech-Hindi_Train.csv\"\n",
    "VALID_PATH = r\"C:\\Users\\sagni\\Downloads\\Tox Watch Hinglish\\archive\\Hatespeech-Hindi_Valid.csv\"\n",
    "TEST_PATH  = r\"C:\\Users\\sagni\\Downloads\\Tox Watch Hinglish\\archive\\Hatespeech-Hindi_Test.csv\"\n",
    "OUT_DIR    = r\"C:\\Users\\sagni\\Downloads\\Tox Watch Hinglish\"\n",
    "# --------------------------------\n",
    "\n",
    "TEXT_CANDS = [\n",
    "    \"text\",\"tweet\",\"sentence\",\"content\",\"message\",\"post\",\"comment\",\n",
    "    \"clean_text\",\"utterance\",\"selftext\",\"title\"\n",
    "]\n",
    "LABEL_CANDS = [\n",
    "    \"label\",\"category\",\"class\",\"target\",\"task_1\",\"task_2\",\"subtask_a\",\"hs_label\",\"y\"\n",
    "]\n",
    "\n",
    "def ensure_out_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def load_csv(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"CSV not found: {path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\", errors=\"ignore\")\n",
    "    # If still messy, try latin-1 silently\n",
    "    if df.empty:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=\"latin-1\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "def detect_text_and_label(df: pd.DataFrame) -> Tuple[pd.Series, pd.Series, str, str]:\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # Text\n",
    "    text_col = None\n",
    "    for c in TEXT_CANDS:\n",
    "        if c in cols_lower:\n",
    "            text_col = cols_lower[c]\n",
    "            break\n",
    "    if text_col is None and \"title\" in cols_lower and \"selftext\" in cols_lower:\n",
    "        # Reddit-style concat\n",
    "        t = (df[cols_lower[\"title\"]].fillna(\"\").astype(str) + \" \" +\n",
    "             df[cols_lower[\"selftext\"]].fillna(\"\").astype(str)).str.strip()\n",
    "        text = t\n",
    "        chosen_text_name = \"title+selftext\"\n",
    "    else:\n",
    "        if text_col is None:\n",
    "            # fallback: first object/string column\n",
    "            obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "            if not obj_cols:\n",
    "                raise ValueError(\"Could not detect a text column. Rename one column to 'text'.\")\n",
    "            text_col = obj_cols[0]\n",
    "        text = df[text_col].astype(str)\n",
    "        chosen_text_name = text_col\n",
    "\n",
    "    # Clean text\n",
    "    text = text.fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    # Drop empty rows after cleaning\n",
    "    mask_nonempty = text != \"\"\n",
    "    text = text[mask_nonempty]\n",
    "    df = df.loc[text.index]\n",
    "\n",
    "    # Label\n",
    "    label_col = None\n",
    "    for c in LABEL_CANDS:\n",
    "        if c in cols_lower:\n",
    "            label_col = cols_lower[c]\n",
    "            break\n",
    "    if label_col is None:\n",
    "        # Heuristic: find a low-cardinality column (<= 20 unique) that looks categorical\n",
    "        for c in df.columns:\n",
    "            uniq = pd.Series(df[c].dropna().unique())\n",
    "            if uniq.size <= 20 and (df[c].dtype == object or pd.api.types.is_integer_dtype(df[c])):\n",
    "                label_col = c\n",
    "                break\n",
    "    if label_col is None:\n",
    "        raise ValueError(\"Could not detect a label column. Rename one column to 'label' or add to LABEL_CANDS.\")\n",
    "\n",
    "    labels_raw = df[label_col]\n",
    "    labels_raw = labels_raw.loc[text.index]\n",
    "    return text, labels_raw, chosen_text_name, label_col\n",
    "\n",
    "def build_global_label_map(splits_raw: Dict[str, pd.Series]) -> Dict[Any, int]:\n",
    "    \"\"\"Create a single mapping for all unique labels (strings or ints) across splits.\"\"\"\n",
    "    uniq_values = []\n",
    "    for s in splits_raw.values():\n",
    "        uniq_values.extend(list(pd.Series(s).dropna().unique()))\n",
    "    # Normalize to strings where appropriate to ensure consistent ordering of text labels\n",
    "    # but keep numeric ints as ints if fully numeric\n",
    "    # Strategy: cast to str for sorting consistency, then map back to original token as key\n",
    "    uniq_values = list(dict.fromkeys(uniq_values))  # de-duplicate preserve order\n",
    "    # Sort by string form to be stable/reproducible\n",
    "    uniq_sorted = sorted(uniq_values, key=lambda x: str(x).lower())\n",
    "\n",
    "    label2id = {v: i for i, v in enumerate(uniq_sorted)}\n",
    "    return label2id\n",
    "\n",
    "def apply_label_map(series: pd.Series, label2id: Dict[Any, int]) -> pd.Series:\n",
    "    mapped = series.map(lambda x: label2id.get(x, None))\n",
    "    # Drop rows where label is unknown\n",
    "    mapped = mapped[mapped.notna()].astype(int)\n",
    "    return mapped\n",
    "\n",
    "def class_counts(df: pd.DataFrame) -> Dict[str, int]:\n",
    "    c = df[\"label\"].value_counts().to_dict()\n",
    "    return {str(int(k)): int(v) for k, v in c.items()}\n",
    "\n",
    "def write_h5(path: str, splits: Dict[str, pd.DataFrame]) -> None:\n",
    "    with h5py.File(path, \"w\") as h5:\n",
    "        for split, df in splits.items():\n",
    "            grp = h5.create_group(split)\n",
    "            str_dt = h5py.string_dtype(encoding=\"utf-8\")\n",
    "            texts = df[\"text\"].astype(str).values\n",
    "            labels = df[\"label\"].astype(np.int16).values  # support >2 classes\n",
    "            grp.create_dataset(\"text\", data=texts, dtype=str_dt, compression=\"gzip\")\n",
    "            grp.create_dataset(\"label\", data=labels, dtype=np.int16, compression=\"gzip\")\n",
    "\n",
    "def write_pkl(path: str, splits: Dict[str, pd.DataFrame], meta: Dict[str, Any]) -> None:\n",
    "    payload = {\n",
    "        \"splits\": {k: v.copy() for k, v in splits.items()},\n",
    "        \"meta\": meta\n",
    "    }\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def write_yaml(path: str, meta: Dict[str, Any]) -> None:\n",
    "    if HAVE_YAML:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            yaml.safe_dump(meta, f, sort_keys=False, allow_unicode=True)\n",
    "    else:\n",
    "        # Fallback: JSON string into .yaml file\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(meta, ensure_ascii=False, indent=2))\n",
    "\n",
    "def write_jsonl(path: str, splits: Dict[str, pd.DataFrame]) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for split, df in splits.items():\n",
    "            for _, row in df.iterrows():\n",
    "                rec = {\"split\": split, \"text\": row[\"text\"], \"label\": int(row[\"label\"])}\n",
    "                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    ensure_out_dir(OUT_DIR)\n",
    "\n",
    "    # Load and detect columns for each split\n",
    "    dfs_raw = {\n",
    "        \"train\": load_csv(TRAIN_PATH),\n",
    "        \"valid\": load_csv(VALID_PATH),\n",
    "        \"test\":  load_csv(TEST_PATH)\n",
    "    }\n",
    "\n",
    "    split_texts_raw: Dict[str, pd.Series] = {}\n",
    "    split_labels_raw: Dict[str, pd.Series] = {}\n",
    "    text_cols_used: Dict[str, str] = {}\n",
    "    label_cols_used: Dict[str, str] = {}\n",
    "\n",
    "    for split, df in dfs_raw.items():\n",
    "        text, labels_raw, text_col, label_col = detect_text_and_label(df)\n",
    "        split_texts_raw[split] = text\n",
    "        split_labels_raw[split] = labels_raw.loc[text.index]\n",
    "        text_cols_used[split] = text_col\n",
    "        label_cols_used[split] = label_col\n",
    "\n",
    "    # Build a single label2id mapping across all splits\n",
    "    label2id = build_global_label_map(split_labels_raw)\n",
    "    id2label = {int(v): str(k) for k, v in label2id.items()}\n",
    "\n",
    "    # Apply mapping and build clean dataframes\n",
    "    splits_clean: Dict[str, pd.DataFrame] = {}\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        t = split_texts_raw[split]\n",
    "        y_id = apply_label_map(split_labels_raw[split], label2id)\n",
    "        # Align indices (drop any rows that lost labels after mapping)\n",
    "        aligned = pd.DataFrame({\"text\": t, \"label\": y_id}).dropna()\n",
    "        splits_clean[split] = aligned.reset_index(drop=True)\n",
    "\n",
    "    # Meta/summary\n",
    "    meta = {\n",
    "        \"dataset_name\": \"toxwatch_hinglish\",\n",
    "        \"source_csv\": {\n",
    "            \"train\": TRAIN_PATH,\n",
    "            \"valid\": VALID_PATH,\n",
    "            \"test\": TEST_PATH\n",
    "        },\n",
    "        \"created_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"text_columns_used\": text_cols_used,\n",
    "        \"label_columns_used\": label_cols_used,\n",
    "        \"label2id\": {str(k): int(v) for k, v in label2id.items()},\n",
    "        \"id2label\": {str(k): v for k, v in id2label.items()},\n",
    "        \"sizes\": {k: int(v.shape[0]) for k, v in splits_clean.items()},\n",
    "        \"class_balance\": {split: class_counts(df) for split, df in splits_clean.items()},\n",
    "        \"notes\": \"Labels are mapped consistently across splits using label2id/id2label.\"\n",
    "    }\n",
    "\n",
    "    # Paths\n",
    "    out_h5    = os.path.join(OUT_DIR, \"toxwatch_hinglish.h5\")\n",
    "    out_pkl   = os.path.join(OUT_DIR, \"toxwatch_hinglish.pkl\")\n",
    "    out_yaml  = os.path.join(OUT_DIR, \"toxwatch_config.yaml\")\n",
    "    out_jsonl = os.path.join(OUT_DIR, \"toxwatch_hinglish.jsonl\")\n",
    "    out_sum   = os.path.join(OUT_DIR, \"toxwatch_summary.json\")\n",
    "\n",
    "    # Write artifacts\n",
    "    write_h5(out_h5, splits_clean)\n",
    "    write_pkl(out_pkl, splits_clean, meta)\n",
    "    write_yaml(out_yaml, meta)\n",
    "    write_jsonl(out_jsonl, splits_clean)\n",
    "    with open(out_sum, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Console summary\n",
    "    print(\"=== ToxWatch Hinglish Artifacts Written ===\")\n",
    "    print(f\"H5:    {out_h5}\")\n",
    "    print(f\"PKL:   {out_pkl}\")\n",
    "    print(f\"YAML:  {out_yaml}\")\n",
    "    print(f\"JSONL: {out_jsonl}\")\n",
    "    print(f\"SUM:   {out_sum}\")\n",
    "    print(\"\\nSizes:\", meta[\"sizes\"])\n",
    "    print(\"Class balance:\", meta[\"class_balance\"])\n",
    "    print(\"Label2ID:\", meta[\"label2id\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfac28-d22d-4098-8733-96f040b357e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
